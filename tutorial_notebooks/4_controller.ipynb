{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a customized Controller\n",
    "\n",
    "In Council, **Controllers** are responsible directing application flow. While Council offers built-in Controllers for simpler applications, we are now going to implement a customized Controller that can make full use of the Skills we defined in the previous steps. \n",
    "\n",
    "When implementing a custom Controller, the two major functions that require implementation are `get_plan` and `select_responses`.\n",
    "\n",
    "To start, recall that when we defined both the OutlineWriterSkill and SectionWriterSkill, we implemented `execute` functions that expect specific information to be provided in the `context`. \n",
    "\n",
    "This means that at a high level, this means that we'll want our Controller to do two things:\n",
    "- Manage state variables\n",
    "- Invoke Chains and their Skills with correctly-packed `initial_states`\n",
    "\n",
    "Let's dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from council.contexts import AgentContext, ScoredChatMessage, ChatMessage, ChatMessageKind\n",
    "from council.chains import Chain\n",
    "from council.llm import LLMMessage, LLMBase\n",
    "from council.utils import Option\n",
    "from council.runners import Budget\n",
    "from council.controllers import ControllerBase, ExecutionUnit\n",
    "\n",
    "import logging\n",
    "from string import Template\n",
    "from typing import List, Tuple\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by extending the ControllerBase, which just specifies that we need to implement two functions: `get_plan` and `select_responses`. The constructor parameters `llm`, `response_threshold`, and `top_k_execution_plan` are modelled after Council's built-in LLMController. \n",
    "\n",
    "For this custom Controller, we add three **state variables** for the Controller to manage: `_article`, `_outline`, and `_iteration`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritingAssistantController(ControllerBase):\n",
    "    \"\"\"\n",
    "    A controller that uses an LLM to decide the execution plan\n",
    "    \"\"\"\n",
    "\n",
    "    _llm: LLMBase\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: LLMBase,\n",
    "        response_threshold: float = 0,\n",
    "        top_k_execution_plan: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new instance\n",
    "\n",
    "        Parameters:\n",
    "            llm (LLMBase): the instance of LLM to use\n",
    "            response_threshold (float): a minimum threshold to select a response from its score\n",
    "            top_k_execution_plan (int): maximum number of execution plan returned\n",
    "        \"\"\"\n",
    "        self._llm = llm\n",
    "        self._response_threshold = response_threshold\n",
    "        self._top_k = top_k_execution_plan\n",
    "\n",
    "        # Controller state variables \n",
    "        self._article = \"\"\n",
    "        self._outline = \"\"\n",
    "        self._iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_plan`\n",
    "\n",
    "Next, we'll define the `get_plan` function. The signature for this function is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plan(self, context: AgentContext, chains: List[Chain], budget: Budget) -> List[ExecutionUnit]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, Council's only expectation is that our implementation needs to return a list of `ExecutionUnits`.\n",
    "\n",
    "An `ExecutionUnit` specifies a Chain to be executed, and an **initial state** for the execution. As we'll see, these initial states will allow us to pass the Controller's state variables to the Skills we defined previously.\n",
    "\n",
    "Let's proceed by definining the prompts that our Controller will use to generate its plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are the Controller module for an AI assistant built to write and revise research articles.\"\n",
    "\n",
    "main_prompt_template = Template(\"\"\"\n",
    "# Task Description\n",
    "Your task is to decide how best to write or revise the ARTICLE. Considering the ARTICLE OUTLINE, ARTICLE, and the CONVERSATION HISTORY,\n",
    "use your avaiable CHAINS to decide what steps to take next. You are not responsible for writing any sections,\n",
    "you are only responsible for deciding what to do next. You will delegate work to other agents via CHAINS.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "You may delegate work to one or more CHAINS.\n",
    "Consider the name and description of each chain and decide whether or how you want to use it. \n",
    "Only give instructions to relevant chains.\n",
    "You can decide to invoke the same chain multiple times, with different instructions. \n",
    "Provide chain instructions that are relevant towards completing your TASK.\n",
    "You will also give each chain invocation a score out of 10, so that their execution can be prioritized.\n",
    "\n",
    "## CHAINS (provided as a list of chain names and descriptions)\n",
    "$chains\n",
    "\n",
    "## CONVERSATION HISTORY\n",
    "$conversation_history\n",
    "\n",
    "## ARTCILE OUTLINE\n",
    "$outline\n",
    "\n",
    "## ARTICLE\n",
    "$article\n",
    "\n",
    "# Contoller Decision formatted precisely as: {chain name};{score out of 10};{instructions on a single line}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the `context` and the Controller's state variables to fill in the main prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increment iteration\n",
    "self._iteration += 1\n",
    "\n",
    "# Get the Chain details\n",
    "chain_details = \"\\n \".join(\n",
    "    [f\"name: {c.name}, description: {c.description}\" for c in chains]\n",
    ")\n",
    "\n",
    "# Get the conversation history\n",
    "conversation_history = [f\"{m.kind}: {m.message}\" for m in context.chatHistory.messages]\n",
    "\n",
    "messages = [\n",
    "    LLMMessage.system_message(system_prompt),\n",
    "    LLMMessage.user_message(\n",
    "        main_prompt_template.substitute(\n",
    "            chains=chain_details,\n",
    "            outline=self._outline,\n",
    "            article=self._article,\n",
    "            conversation_history=conversation_history,\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we post a chat request to the LLM with our formatted messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = self._llm.post_chat_request(messages)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to parse the LLM's response and create a plan, i.e. a list of `ExecutionUnits`. \n",
    "\n",
    "We'll first define a class-level helper function `parse_line` to parse each line of the LLM's response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def parse_line(line: str, chains: List[Chain]) -> Option[Tuple[Chain, int, str]]:\n",
    "    result: Option[Tuple[Chain, int]] = Option.none()\n",
    "    try:\n",
    "        (name, score, instruction) = line.split(\";\")[:3]\n",
    "        chain = next(filter(lambda item: item.name == name, chains))\n",
    "        result = Option.some((chain, int(score), instruction))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Controller parsing error: {e}.\\n{line}\")\n",
    "    finally:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we proceed with the implementation of the `get_plan` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = response.splitlines()\n",
    "parsed = [p for p in parsed if len(p) > 0]\n",
    "parsed = [self.parse_line(line, chains) for line in parsed] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing each line in of the LLM's response, we filter responses based on the Controllers response threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [\n",
    "    r.unwrap() for r in parsed if r.is_some() and r.unwrap()[1] > self._response_threshold\n",
    "]\n",
    "\n",
    "if (filtered is None) or (len(filtered) == 0):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the filtered, parsed responses are non-empty, we complete the implementation by creating a list of `ExecutionUnits`.\n",
    "\n",
    "Notice, in particular, how we are preparing `initial_state`. Since every ChatMessage has a required `message` parameter, we'll use this to carry the Controller's instructions for each Chain in its plan. We can then pack the other state variables into the `data` parameter using a dictionary.\n",
    "\n",
    "In your own custom Controller, you could choose to use the `message` and `data` fields differently. For example, you could choose to include the instructions in the `data` object instead of using `message`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.sort(key=lambda item: item[1], reverse=True)\n",
    "\n",
    "result = []\n",
    "for chain, score, instruction in filtered: \n",
    "\n",
    "    # Prepare initial state to be readable by\n",
    "    # first Skills in an invoked chain.\n",
    "    initial_state = ChatMessage.chain(\n",
    "        message=instruction, \n",
    "        data={\n",
    "            \"article\": self._article, \n",
    "            \"outline\": self._outline, \n",
    "            \"iteration\": self._iteration\n",
    "        }\n",
    "    )\n",
    "\n",
    "    exec_unit = ExecutionUnit(\n",
    "        chain,\n",
    "        budget,\n",
    "        initial_state,\n",
    "        name=f\"{chain.name}: {instruction}\"\n",
    "    )\n",
    "    result.append(exec_unit)\n",
    "\n",
    "result = result[: self._top_k]\n",
    "return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `select_responses`\n",
    "\n",
    "Now we will implement the `select_responses` function. This function will be invoked automatically after a plan has been executed and the responses have been evaluated. This function defines the last step performed by an Agent before either: \n",
    "1. A response is returned to the calling agent or user\n",
    "2. No response is selected and the Agent will continue handling the request by generating a new plan\n",
    "\n",
    "In this example application, we'll leverage Council's flexibility to add highly customized logic here. Since we may be invoking the same Chain multiple times in parallel using different instructions, each response may contain a different part of the article we want to return to the user. To work towards a complete article, we can add logic here to not only select, but also *aggregate* multiple responses. \n",
    "\n",
    "We'll start by accessing all the reponses from the most recent evaluation round and filter down to the ones that were generated *in the most recent Controller iteration*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_responses(self, context: AgentContext) -> List[ScoredChatMessage]:\n",
    "\n",
    "    \"\"\"\n",
    "    Aggregation phase. \n",
    "\n",
    "    Get latest iteration results from Evaluator and aggregate if applicable.\n",
    "    \"\"\"\n",
    "\n",
    "    all_eval_results = sorted(context.evaluationHistory[-1], key=lambda x: x.score, reverse=True)\n",
    "    current_iteration_results = []\n",
    "    for scored_result in all_eval_results:\n",
    "        message = scored_result.message\n",
    "        if message.data['iteration'] == self._iteration:\n",
    "            current_iteration_results.append(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define two aggregation steps, one for article outlines and another for article sections. First we partition responses based on the Skill that generated them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If multiple outlines or articles were generated in the last iteration, \n",
    "## use LLM calls to aggregate them.\n",
    "\n",
    "outlines = []\n",
    "articles = []\n",
    "chat_history = [f\"{m.kind}: {m.message}\" for m in context.chatHistory.messages]\n",
    "\n",
    "for message in current_iteration_results:\n",
    "    source = message.source\n",
    "    if source == \"SectionWriterSkill\":\n",
    "        articles.append(message.data['article'])\n",
    "    elif source == \"OutlineWriterSkill\":\n",
    "        outlines.append(message.data['outline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use an LLM call to aggregate outlines, if there were more than one.\n",
    "\n",
    "### Outline Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert-level AI writing editor. \n",
    "Your role is to aggregate multiple suggestions for an article outline into a single one.\"\"\"\n",
    "\n",
    "main_prompt_template = Template(\"\"\"\n",
    "# Task Description\n",
    "Your task is to combine one or more article outlines into a single one written in markdown format.\n",
    "\n",
    "# Instructions\n",
    "Read the CHAT HISTORY and POSSIBLE OUTLINES. Then respond with a single article outline that best matches what is being requested in the CHAT HISTORY.\n",
    "\n",
    "## CHAT HISTORY\n",
    "$chat_history\n",
    "\n",
    "## POSSIBLE OUTLINES\n",
    "$possible_outlines\n",
    "\n",
    "## OUTLINE\n",
    "\"\"\")\n",
    "\n",
    "if len(outlines) > 1:\n",
    "    messages = [\n",
    "        LLMMessage.system_message(system_prompt),\n",
    "        LLMMessage.user_message(\n",
    "            main_prompt_template.substitute(\n",
    "                chat_history=chat_history,\n",
    "                possible_outlines = outlines\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    "    response = self._llm.post_chat_request(messages)[0]\n",
    "    self._outline = response\n",
    "elif len(outlines) == 1:\n",
    "    self._outline = outlines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we use another LLM call to aggregate article sections, if there were more than one.\n",
    "\n",
    "### Article Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an expert-level AI writing editor. Your role is to aggregate multiple partial articles into a single, complete article.\"\n",
    "main_prompt_template = Template(\"\"\"\n",
    "# Task Description\n",
    "Your task is to combine one or more partial articles into a single one written in markdown format.\n",
    "\n",
    "# Instructions\n",
    "Read the CHAT HISTORY and PARTIAL ARTICLES. Then respond with a single article that best matches what is being requested in the CHAT HISTORY.\n",
    "\n",
    "## CHAT HISTORY\n",
    "$chat_history\n",
    "\n",
    "## PARTIAL ARTICLES\n",
    "$partial_articles\n",
    "\n",
    "## ARTICLE\n",
    "\"\"\")\n",
    "\n",
    "if len(articles) > 1:\n",
    "    messages = [\n",
    "        LLMMessage.system_message(system_prompt),\n",
    "        LLMMessage.user_message(\n",
    "            main_prompt_template.substitute(\n",
    "                chat_history=chat_history,\n",
    "                partial_articles = articles\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    "    response = self._llm.post_chat_request(messages)[0]\n",
    "    self._article = response\n",
    "elif len(articles) == 1:\n",
    "    self._article = articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in our implementation of `select_responses` is to determine whether to return the (aggregated) article to the user or to go for another iteration. \n",
    "\n",
    "### Iteration Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decide whether to keep iterating or to return the article\n",
    "### to the user for review.\n",
    "\n",
    "system_prompt = \"\"\"You are an expert-level AI writing editor. \n",
    "Your role is to decide whether to keep editing the ARTICLE.\"\"\"\n",
    "\n",
    "main_prompt_template = Template(\"\"\"\n",
    "# Task Description\n",
    "Your task is to decide whether:\n",
    "1. To keep editing the ARTICLE, or\n",
    "2. To return the article to the requesting agent.\n",
    "\n",
    "# Instructions\n",
    "Read the ARTICLE and CHAT HISTORY then consider the following instructions:\n",
    "- If the ARTICLE still has placeholders or empty sections, KEEP EDITING.\n",
    "- If the ARTICLE is incoherent, KEEP EDITING.\n",
    "- If the ARTICLE does not include everything being requested in the CHAT HISTORY, KEEP EDITING.\n",
    "- If the ARTICLE does not include every section in ARTICLE OUTLINE, KEEP EDITING.\n",
    "- The ARTICLE is only COMPLETE when it covers every section and subsection in the ARTICLE OUTLINE.\n",
    "- If the ARTICLE is COMPLETE, RETURN TO REQUESTING AGENT.\n",
    "\n",
    "## ARTCILE OUTLINE\n",
    "$outline\n",
    "\n",
    "## ARTICLE\n",
    "$article\n",
    "\n",
    "## CHAT HISTORY\n",
    "$chat_history\n",
    "\n",
    "# Your Response (exactly one of [\"KEEP EDITING\", \"RETURN TO REQUESTING AGENT\"])\n",
    "\"\"\")\n",
    "\n",
    "messages = [\n",
    "    LLMMessage.system_message(system_prompt),\n",
    "    LLMMessage.user_message(\n",
    "        main_prompt_template.substitute(\n",
    "            article=self._article,\n",
    "            outline=self._outline,\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = self._llm.post_chat_request(messages)[0]\n",
    "logger.debug(f\"llm response: {response}\")\n",
    "\n",
    "if \"KEEP EDITING\" in response:\n",
    "    return []\n",
    "else:\n",
    "    return [ScoredChatMessage(ChatMessage(message=self._article, kind=ChatMessageKind.Agent), 1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Controller implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from council.contexts import AgentContext, ScoredChatMessage, ChatMessage, ChatMessageKind\n",
    "from council.chains import Chain\n",
    "from council.llm import LLMMessage, LLMBase\n",
    "from council.utils import Option\n",
    "from council.runners import Budget\n",
    "from council.controllers import ControllerBase, ExecutionUnit\n",
    "\n",
    "import logging\n",
    "from string import Template\n",
    "from typing import List, Tuple\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WritingAssistantController(ControllerBase):\n",
    "    \"\"\"\n",
    "    A controller that uses an LLM to decide the execution plan\n",
    "    \"\"\"\n",
    "\n",
    "    _llm: LLMBase\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: LLMBase,\n",
    "        response_threshold: float = 0,\n",
    "        top_k_execution_plan: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new instance\n",
    "\n",
    "        Parameters:\n",
    "            llm (LLMBase): the instance of LLM to use\n",
    "            response_threshold (float): a minimum threshold to select a response from its score\n",
    "            top_k_execution_plan (int): maximum number of execution plan returned\n",
    "        \"\"\"\n",
    "        self._llm = llm\n",
    "        self._response_threshold = response_threshold\n",
    "        self._top_k = top_k_execution_plan\n",
    "\n",
    "        # Controller state variables \n",
    "        self._article = \"\"\n",
    "        self._outline = \"\"\n",
    "        self._iteration = 0\n",
    "\n",
    "    def get_plan(\n",
    "        self, context: AgentContext, chains: List[Chain], budget: Budget\n",
    "    ) -> List[ExecutionUnit]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Planning phase.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = \"You are the Controller module for an AI assistant built to write and revise research articles.\"\n",
    "\n",
    "        main_prompt_template = Template(\"\"\"\n",
    "        # Task Description\n",
    "        Your task is to decide how best to write or revise the ARTICLE. Considering the ARTICLE OUTLINE, ARTICLE, and the CONVERSATION HISTORY,\n",
    "        use your avaiable CHAINS to decide what steps to take next. You are not responsible for writing any sections,\n",
    "        you are only responsible for deciding what to do next. You will delegate work to other agents via CHAINS.\n",
    "\n",
    "        # Instructions\n",
    "\n",
    "        You may delegate work to one or more CHAINS.\n",
    "        Consider the name and description of each chain and decide whether or how you want to use it. \n",
    "        Only give instructions to relevant chains.\n",
    "        You can decide to invoke the same chain multiple times, with different instructions. \n",
    "        Provide chain instructions that are relevant towards completing your TASK.\n",
    "        You will also give each chain invocation a score out of 10, so that their execution can be prioritized.\n",
    "\n",
    "        ## CHAINS (provided as a list of chain names and descriptions)\n",
    "        $chains\n",
    "\n",
    "        ## CONVERSATION HISTORY\n",
    "        $conversation_history\n",
    "\n",
    "        ## ARTCILE OUTLINE\n",
    "        $outline\n",
    "\n",
    "        ## ARTICLE\n",
    "        $article\n",
    "\n",
    "        # Contoller Decision formatted precisely as: {chain name};{score out of 10};{instructions on a single line}\n",
    "        \"\"\")\n",
    "\n",
    "        # Increment iteration\n",
    "        self._iteration += 1\n",
    "\n",
    "        # Get the Chain details\n",
    "        chain_details = \"\\n \".join(\n",
    "            [f\"name: {c.name}, description: {c.description}\" for c in chains]\n",
    "        )\n",
    "\n",
    "        # Get the conversation history\n",
    "        conversation_history = [f\"{m.kind}: {m.message}\" for m in context.chatHistory.messages]\n",
    "\n",
    "        messages = [\n",
    "            LLMMessage.system_message(system_prompt),\n",
    "            LLMMessage.user_message(\n",
    "                main_prompt_template.substitute(\n",
    "                    chains=chain_details,\n",
    "                    outline=self._outline,\n",
    "                    article=self._article,\n",
    "                    conversation_history=conversation_history,\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        response = self._llm.post_chat_request(messages)[0]\n",
    "        logger.debug(f\"llm response: {response}\")\n",
    "\n",
    "        parsed = response.splitlines()\n",
    "        parsed = [p for p in parsed if len(p) > 0]\n",
    "        parsed = [self.parse_line(line, chains) for line in parsed]\n",
    "\n",
    "        filtered = [\n",
    "            r.unwrap()\n",
    "            for r in parsed\n",
    "            if r.is_some() and r.unwrap()[1] > self._response_threshold\n",
    "        ]\n",
    "        if (filtered is None) or (len(filtered) == 0):\n",
    "            return []\n",
    "\n",
    "        filtered.sort(key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        result = []\n",
    "        for chain, score, instruction in filtered: \n",
    "            initial_state = ChatMessage.chain(\n",
    "                message=instruction, data={\"article\": self._article, \"outline\": self._outline, \"iteration\": self._iteration}\n",
    "            )\n",
    "            exec_unit = ExecutionUnit(\n",
    "                chain,\n",
    "                budget,\n",
    "                initial_state=initial_state,\n",
    "                name=f\"{chain.name}: {instruction}\"\n",
    "            )\n",
    "            result.append(exec_unit)\n",
    "\n",
    "        result = result[: self._top_k]\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_line(line: str, chains: List[Chain]) -> Option[Tuple[Chain, int, str]]:\n",
    "        result: Option[Tuple[Chain, int]] = Option.none()\n",
    "        try:\n",
    "            (name, score, instruction) = line.split(\";\")[:3]\n",
    "            chain = next(filter(lambda item: item.name == name, chains))\n",
    "            result = Option.some((chain, int(score), instruction))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logger.error(f\"Controller parsing error: {e}.\\n{line}\")\n",
    "        finally:\n",
    "            return result\n",
    "        \n",
    "    def select_responses(self, context: AgentContext) -> List[ScoredChatMessage]:\n",
    "\n",
    "        \"\"\"\n",
    "        Aggregation phase. \n",
    "\n",
    "        Get latest iteration results from Evaluator and aggregate if applicable.\n",
    "        \"\"\"\n",
    "\n",
    "        all_eval_results = sorted(context.evaluationHistory[-1], key=lambda x: x.score, reverse=True)\n",
    "        current_iteration_results = []\n",
    "        for scored_result in all_eval_results:\n",
    "            message = scored_result.message\n",
    "            if message.data['iteration'] == self._iteration:\n",
    "                current_iteration_results.append(message)\n",
    "\n",
    "        ## If multiple outlines or articles were generated in the last iteration, \n",
    "        ## use LLM calls to aggregate them.\n",
    "\n",
    "        outlines = []\n",
    "        articles = []\n",
    "        conversation_history = [f\"{m.kind}: {m.message}\" for m in context.chatHistory.messages]\n",
    "\n",
    "        for message in current_iteration_results:\n",
    "            source = message.source\n",
    "            if source == \"SectionWriterSkill\":\n",
    "                articles.append(message.data['article'])\n",
    "            elif source == \"OutlineWriterSkill\":\n",
    "                outlines.append(message.data['outline'])\n",
    "\n",
    "        ### Outline Aggregation\n",
    "\n",
    "        system_prompt = \"You are an expert-level AI writing editor. Your role is to aggregate multiple suggestions for an article outline into a single one.\"\n",
    "        main_prompt_template = Template(\"\"\"\n",
    "        # Task Description\n",
    "        Your task is to combine one or more article outlines into a single one written in markdown format.\n",
    "\n",
    "        # Instructions\n",
    "        Read the CHAT HISTORY and POSSIBLE OUTLINES. Then respond with a single article outline that best matches what is being requested in the CHAT HISTORY.\n",
    "\n",
    "        ## CONVERSATION HISTORY\n",
    "        $conversation_history\n",
    "\n",
    "        ## POSSIBLE OUTLINES\n",
    "        $possible_outlines\n",
    "\n",
    "        ## OUTLINE\n",
    "        \"\"\")\n",
    "\n",
    "        if len(outlines) > 1:\n",
    "            messages = [\n",
    "                LLMMessage.system_message(system_prompt),\n",
    "                LLMMessage.user_message(\n",
    "                    main_prompt_template.substitute(\n",
    "                        conversation_history=conversation_history,\n",
    "                        possible_outlines = outlines\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "            response = self._llm.post_chat_request(messages)[0]\n",
    "            self._outline = response\n",
    "\n",
    "        ### Article Aggregation\n",
    "\n",
    "        system_prompt = \"You are an expert-level AI writing editor. Your role is to aggregate multiple partial articles into a single, complete article.\"\n",
    "        main_prompt_template = Template(\"\"\"\n",
    "        # Task Description\n",
    "        Your task is to combine one or more partial articles into a single one written in markdown format.\n",
    "\n",
    "        # Instructions\n",
    "        Read the CHAT HISTORY and PARTIAL ARTICLES. Then respond with a single article that best matches what is being requested in the CHAT HISTORY.\n",
    "\n",
    "        ## CONVERSATION HISTORY\n",
    "        $conversation_history\n",
    "\n",
    "        ## PARTIAL ARTICLES\n",
    "        $partial_articles\n",
    "\n",
    "        ## ARTICLE\n",
    "        \"\"\")\n",
    "\n",
    "        if len(articles) > 1:\n",
    "            messages = [\n",
    "                LLMMessage.system_message(system_prompt),\n",
    "                LLMMessage.user_message(\n",
    "                    main_prompt_template.substitute(\n",
    "                        conversation_history=conversation_history,\n",
    "                        partial_articles = articles\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "            response = self._llm.post_chat_request(messages)[0]\n",
    "            self._article = response\n",
    "\n",
    "        ### Decide whether to keep iterating or to return the article\n",
    "        ### to the user for review.\n",
    "\n",
    "        system_prompt = \"You are an expert-level AI writing editor. Your role is to decide whether to keep editing the ARTICLE.\"\n",
    "        main_prompt_template = Template(\"\"\"\n",
    "        # Task Description\n",
    "        Your task is to decide whether:\n",
    "        1. To keep editing the ARTICLE, or\n",
    "        2. To return the article to the requesting agent.\n",
    "\n",
    "        # Instructions\n",
    "        Read the ARTICLE and CHAT HISTORY then consider the following instructions:\n",
    "        - If the ARTICLE still has placeholders or empty sections, KEEP EDITING.\n",
    "        - If the ARTICLE is incoherent, KEEP EDITING.\n",
    "        - If the ARTICLE does not include everything being requested in the CHAT HISTORY, KEEP EDITING.\n",
    "        - If the ARTICLE does not include every section in ARTICLE OUTLINE, KEEP EDITING.\n",
    "        - The ARTICLE is only COMPLETE when it covers every section and subsection in the ARTICLE OUTLINE.\n",
    "        - If the ARTICLE is COMPLETE, RETURN TO REQUESTING AGENT.\n",
    "\n",
    "        ## ARTCILE OUTLINE\n",
    "        $outline\n",
    "\n",
    "        ## ARTICLE\n",
    "        $article\n",
    "\n",
    "        ## CONVERSATION HISTORY\n",
    "        $conversation_history\n",
    "\n",
    "        # Your Response (exactly one of [\"KEEP EDITING\", \"RETURN TO REQUESTING AGENT\"])\n",
    "        \"\"\")\n",
    "\n",
    "        messages = [\n",
    "            LLMMessage.system_message(system_prompt),\n",
    "            LLMMessage.user_message(\n",
    "                main_prompt_template.substitute(\n",
    "                    article=self._article,\n",
    "                    outline=self._outline,\n",
    "                    conversation_history=conversation_history,\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        response = self._llm.post_chat_request(messages)[0]\n",
    "        logger.debug(f\"llm response: {response}\")\n",
    "\n",
    "        if \"KEEP EDITING\" in response:\n",
    "            return []\n",
    "        else:\n",
    "            return [ScoredChatMessage(ChatMessage(message=self._article, kind=ChatMessageKind.Agent), 1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We're almost ready to build and interact with our WritingAssistantAgent. Before we do that, we just need to implement a (minimally) customized [Evaluator](./5_evaluator.md)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
